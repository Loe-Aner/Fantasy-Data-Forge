{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe3d7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper_wiki import pobierz_soup\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook, load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b1d36c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odczytano 4 wierszy z: D:\\MyProjects_4Fun\\projects\\World of Warcraft\\excel-mappingi\\surowe\\wowhead_id_kraina_dodatek.xlsx [prawie_gotowe_dane]\n",
      "Po odfiltrowaniu pustych URL: 4 wierszy\n",
      "W mapping_01 jest już 2 URL-i\n",
      "Nowych wierszy do pobrania: 2\n",
      "[1/2] Scrapuję: https://www.wowhead.com/quest=86881\n",
      "[1/2] OK: storyline='Call of the Goddess' | patch='12.0.0'\n",
      "[2/2] Scrapuję: https://www.wowhead.com/quest=90777\n",
      "[2/2] OK: storyline='Foothold' | patch='12.0.0'\n",
      "Zapisano ostatnią paczkę 2 wierszy od wiersza 4\n",
      "Koniec. Dopisano: 2, błędy: 0, plik: D:\\MyProjects_4Fun\\projects\\World of Warcraft\\excel-mappingi\\mapping_01.xlsx\n"
     ]
    }
   ],
   "source": [
    "raw_path = r\"D:\\MyProjects_4Fun\\projects\\World of Warcraft\\excel-mappingi\\surowe\\wowhead_id_kraina_dodatek.xlsx\"\n",
    "out_path = r\"D:\\MyProjects_4Fun\\projects\\World of Warcraft\\excel-mappingi\\mapping_01.xlsx\"\n",
    "\n",
    "input_sheet = \"prawie_gotowe_dane\"\n",
    "output_sheet = \"mapping_01\"\n",
    "url_col = \"MISJA_URL_WOWHEAD\"\n",
    "\n",
    "def wyciagnij_patch(soup):\n",
    "    for s in soup.find_all(\"script\"):\n",
    "        t = s.get_text(\" \", strip=True)\n",
    "        if \"Added in patch\" not in t:\n",
    "            continue\n",
    "        m = re.search(r'Added in patch\\s*\\[acronym=\\\\?\"[^\"]*\\\\?\"\\]([0-9]+\\.[0-9]+\\.[0-9]+)\\[\\\\?/acronym\\]', t)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "    return \"\"\n",
    "\n",
    "def normalize_cell(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, float) and pd.isna(v):\n",
    "        return None\n",
    "    if pd.isna(v):\n",
    "        return None\n",
    "    return v\n",
    "\n",
    "df_raw = pd.read_excel(raw_path, sheet_name=input_sheet)\n",
    "print(f\"Odczytano {len(df_raw)} wierszy z: {raw_path} [{input_sheet}]\")\n",
    "\n",
    "if url_col not in df_raw.columns:\n",
    "    raise ValueError(f\"Brak kolumny {url_col} w arkuszu {input_sheet}\")\n",
    "\n",
    "df_raw[url_col] = (\n",
    "    df_raw[url_col]\n",
    "      .astype(str)\n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "df_raw = df_raw[df_raw[url_col].notna() & (df_raw[url_col] != \"\")].copy()\n",
    "print(f\"Po odfiltrowaniu pustych URL: {len(df_raw)} wierszy\")\n",
    "\n",
    "headers = list(df_raw.columns) + [\"storyline\", \"patch\"]\n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = output_sheet\n",
    "    ws.append(headers)\n",
    "    wb.save(out_path)\n",
    "    print(f\"Utworzono plik wynikowy: {out_path} [{output_sheet}]\")\n",
    "else:\n",
    "    wb = load_workbook(out_path)\n",
    "    if output_sheet not in wb.sheetnames:\n",
    "        ws = wb.create_sheet(output_sheet)\n",
    "        ws.append(headers)\n",
    "        wb.save(out_path)\n",
    "        print(f\"Dodano arkusz: {output_sheet} do {out_path}\")\n",
    "    else:\n",
    "        ws = wb[output_sheet]\n",
    "        first_row = [ws.cell(row=1, column=i + 1).value for i in range(ws.max_column)]\n",
    "        if first_row[:len(headers)] != headers:\n",
    "            print(\"Uwaga: Nagłówki w mapping_01 różnią się od źródła. Dopiszę wiersze wg bieżących nagłówków mapping_01.\")\n",
    "            headers = [h for h in first_row if h is not None]\n",
    "\n",
    "df_out = pd.read_excel(out_path, sheet_name=output_sheet)\n",
    "if url_col in df_out.columns:\n",
    "    existing_urls = set(\n",
    "        df_out[url_col]\n",
    "          .dropna()\n",
    "          .astype(str)\n",
    "          .str.strip()\n",
    "          .tolist()\n",
    "    )\n",
    "else:\n",
    "    existing_urls = set()\n",
    "\n",
    "print(f\"W mapping_01 jest już {len(existing_urls)} URL-i\")\n",
    "\n",
    "df_new = df_raw[~df_raw[url_col].isin(existing_urls)].copy()\n",
    "print(f\"Nowych wierszy do pobrania: {len(df_new)}\")\n",
    "\n",
    "if df_new.empty:\n",
    "    print(\"Nic do roboty — wszystko już jest w mapping_01.\")\n",
    "else:\n",
    "    wb = load_workbook(out_path)\n",
    "    ws = wb[output_sheet]\n",
    "\n",
    "    batch_size = 200\n",
    "    bufor = []\n",
    "    dopisane = 0\n",
    "    bledy = 0\n",
    "\n",
    "    for idx, row in enumerate(df_new.itertuples(index=False), start=1):\n",
    "        row_dict = row._asdict()\n",
    "        link = str(row_dict.get(url_col, \"\")).strip()\n",
    "\n",
    "        print(f\"[{idx}/{len(df_new)}] Scrapuję: {link}\")\n",
    "        soup = pobierz_soup(link, parser=\"lxml\")\n",
    "        if soup is None:\n",
    "            bledy += 1\n",
    "            print(f\"[{idx}/{len(df_new)}] Błąd pobierania: {link}\")\n",
    "            continue\n",
    "\n",
    "        element = soup.select_one(\".quick-facts-storyline-title\")\n",
    "        if element is None:\n",
    "            bledy += 1\n",
    "            print(f\"[{idx}/{len(df_new)}] Brak storyline: {link}\")\n",
    "            continue\n",
    "\n",
    "        storyline = element.get_text().strip()\n",
    "        patch = wyciagnij_patch(soup)\n",
    "\n",
    "        row_dict[\"storyline\"] = storyline\n",
    "        row_dict[\"patch\"] = patch\n",
    "\n",
    "        out_row = [normalize_cell(row_dict.get(col)) for col in headers]\n",
    "        bufor.append(out_row)\n",
    "\n",
    "        print(f\"[{idx}/{len(df_new)}] OK: storyline='{storyline}' | patch='{patch}'\")\n",
    "\n",
    "        if len(bufor) >= batch_size:\n",
    "            start_row_excel = ws.max_row + 1\n",
    "            for r in bufor:\n",
    "                ws.append(r)\n",
    "            wb.save(out_path)\n",
    "            dopisane += len(bufor)\n",
    "            print(f\"Zapisano paczkę {len(bufor)} wierszy od wiersza {start_row_excel}\")\n",
    "            bufor = []\n",
    "\n",
    "        time.sleep(random.uniform(1.3, 1.9))\n",
    "\n",
    "    if bufor:\n",
    "        start_row_excel = ws.max_row + 1\n",
    "        for r in bufor:\n",
    "            ws.append(r)\n",
    "        wb.save(out_path)\n",
    "        dopisane += len(bufor)\n",
    "        print(f\"Zapisano ostatnią paczkę {len(bufor)} wierszy od wiersza {start_row_excel}\")\n",
    "\n",
    "    print(f\"Koniec. Dopisano: {dopisane}, błędy: {bledy}, plik: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef1fb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
